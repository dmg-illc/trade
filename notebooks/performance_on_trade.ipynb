{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "from os.path import join\n",
    "# import sys\n",
    "# sys.path.insert(0, '/Users/anna/Documents/Code/ads-official/')\n",
    "# from bert_utils import *\n",
    "from src.paths import MAIN_DIR_PATH, OUTPUT_FOLDER, TRADE_PATH, DATA_FOLDER\n",
    "from src.utils import open_json\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on TRADE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_outs = pickle.load(open(OUTPUT_FOLDER / 'clip/clip-vit-large-patch14-336_clip_score_trade.pkl', \"rb\"))['emb_dict']\n",
    "trade = pd.read_csv(TRADE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP achieves an accuracy of: 0.35\n"
     ]
    }
   ],
   "source": [
    "logits = np.vstack([clip_outs[k]['logits'] for k in clip_outs])\n",
    "n, _ = logits.shape\n",
    "print(f'CLIP achieves an accuracy of: {round((logits.argmax(axis=1)==0).sum().item()/n,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "align_outs = pickle.load(open(OUTPUT_FOLDER / 'align/align-base_align_score_trade.pkl', \"rb\"))['emb_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALIGN achieves an accuracy of: 0.28\n"
     ]
    }
   ],
   "source": [
    "logits = np.vstack([align_outs[k]['logits'] for k in clip_outs])\n",
    "n, _ = logits.shape\n",
    "print(f'ALIGN achieves an accuracy of: {round((logits.argmax(axis=1)==0).sum().item()/n,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALBEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = pickle.load(open(OUTPUT_FOLDER / 'albef/albef_retrieval_albef_score_trade.pkl', \"rb\"))\n",
    "images = np.vstack([embs['emb_dict'][k]['image_embedding'] for k in embs['emb_dict']])\n",
    "ar = np.vstack([embs['emb_dict'][k]['text_embedding'][0,:] for k in embs['emb_dict']])\n",
    "d1 = np.vstack([embs['emb_dict'][k]['text_embedding'][1,:] for k in embs['emb_dict']])\n",
    "d2 = np.vstack([embs['emb_dict'][k]['text_embedding'][2,:] for k in embs['emb_dict']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3, 256)\n",
      "(300, 3)\n",
      "ALBEF achieves an accuracy of: 0.33\n"
     ]
    }
   ],
   "source": [
    "n = images.shape[0]\n",
    "emb_dim = images.shape[1]\n",
    "cands = np.stack([ar, d1, d2], axis=1)\n",
    "print(cands.shape)\n",
    "temperature = 0.0045\n",
    "end_mat = (np.matmul(images.reshape(n, 1, emb_dim),\n",
    "          np.transpose(cands, (0,2,1))).reshape(n, 3)/temperature)\n",
    "print(end_mat.shape)\n",
    "print(f'ALBEF achieves an accuracy of: {round((np.argmax(end_mat,axis=1)==0).sum()/n,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['images', 'ar', 'dist1', 'dist2', 'model_checkpoint', 'temperature'])\n"
     ]
    }
   ],
   "source": [
    "embs = torch.load(join(OUTPUT_FOLDER, 'lit/lit_outputs_trade'), map_location=torch.device('cpu'))\n",
    "print(embs.keys())\n",
    "images = embs['images'].to(torch.float32)\n",
    "ar = embs['ar'].to(torch.float32)\n",
    "d1 = embs['dist1'].to(torch.float32)\n",
    "d2 = embs['dist2'].to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 3])\n",
      "LiT achieves an accuracy of: 0.31\n"
     ]
    }
   ],
   "source": [
    "n = images.size()[0]\n",
    "emb_dim = images.size()[1]\n",
    "cands = torch.stack((ar, d1, d2), dim=1)\n",
    "\n",
    "end_mat = torch.bmm(images.view((n,1,emb_dim)), \n",
    "                    cands.transpose(1,2)).view(n,3)\n",
    "print(end_mat.size())\n",
    "print(f'LiT achieves an accuracy of: {round((end_mat.argmax(dim=1)==0).sum().item()/n,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on TRADE control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance(ar_mat, im_mat, control_dict):\n",
    "\n",
    "    ar_mat = torch.tensor(ar_mat.tolist())\n",
    "    im_mat = torch.tensor(im_mat.tolist())\n",
    "    n, emb_dim = ar_mat.shape\n",
    "    acc_arr = torch.zeros(10)\n",
    "\n",
    "    cands = torch.zeros(10*n, 3, emb_dim)\n",
    "    for i in range(10):\n",
    "\n",
    "        d1, d2 = ar_mat[control_dict[f'split_{i+1}']['dist1']], ar_mat[control_dict[f'split_{i+1}']['dist2']]\n",
    "        curr_cands = torch.stack((ar_mat, d1, d2), dim=1)\n",
    "        cands[(n*i):(n*(i+1))] = curr_cands\n",
    "        end_mat = torch.bmm(im_mat.view((n,1,emb_dim)), \n",
    "                            curr_cands.transpose(1,2)).view(n,3)\n",
    "        acc_arr[i] = (end_mat.argmax(dim=1)==0).sum().item()/n\n",
    "\n",
    "    end_mat = torch.bmm(im_mat.repeat(10,1).view((n*10,1,emb_dim)), \n",
    "                            cands.transpose(1,2)).view(n*10,3)\n",
    "    rank = ((torch.argsort(end_mat, dim=1, descending=True)+1)[:,0]).to(torch.float32)\n",
    "    print(f'Acc: {round(acc_arr.mean().item(), 2)} ({round(acc_arr.std().item(), 2)})')\n",
    "    print(f'Rank: {round(rank.mean().item(), 2)} ({round(rank.std().item(), 2)})')\n",
    "  \n",
    "    \n",
    "    # return round(acc_arr.mean().item(),2), round(acc_arr.std().item(),2), round(rank_arr.mean().item(),2), round(std_rank_arr.mean().item(),2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_dict = open_json(DATA_FOLDER / 'TRADE' / 'trade_control.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.98 (0.01)\n",
      "Rank: 1.03 (0.21)\n"
     ]
    }
   ],
   "source": [
    "# CLIP\n",
    "embs = pickle.load(open(OUTPUT_FOLDER /'clip/clip-vit-large-patch14-336_clip_score_trade.pkl', \"rb\"))['emb_dict']\n",
    "images = np.stack([embs[k]['image_embedding'].flatten() for k in embs])\n",
    "ar = np.stack([embs[k]['text_embedding'][0].flatten() for k in embs])\n",
    "get_variance(ar, images, control_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.97 (0.01)\n",
      "Rank: 1.04 (0.26)\n"
     ]
    }
   ],
   "source": [
    "# ALIGN\n",
    "embs = pickle.load(open(OUTPUT_FOLDER /'align/align-base_align_score_trade.pkl', \"rb\"))['emb_dict']\n",
    "images = np.stack([embs[k]['image_embedding'].flatten() for k in embs])\n",
    "ar = np.stack([embs[k]['text_embedding'][0].flatten() for k in embs])\n",
    "get_variance(ar, images, control_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.87 (0.01)\n",
      "Rank: 1.19 (0.53)\n"
     ]
    }
   ],
   "source": [
    "# ALBEF\n",
    "embs = pickle.load(open(OUTPUT_FOLDER / 'albef/albef_retrieval_albef_score_trade.pkl', \"rb\"))['emb_dict']\n",
    "images = np.vstack([embs[k]['image_embedding'] for k in embs])\n",
    "ar = np.vstack([embs[k]['text_embedding'][0,:] for k in embs])\n",
    "get_variance(ar, images, control_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.82 (0.02)\n",
      "Rank: 1.26 (0.6)\n"
     ]
    }
   ],
   "source": [
    "# LiT\n",
    "\n",
    "embs = torch.load(join(OUTPUT_FOLDER, 'lit/lit_outputs_trade'), map_location=torch.device('cpu'))\n",
    "\n",
    "images = embs['images'].to(torch.float32)\n",
    "ar = embs['ar'].to(torch.float32)\n",
    "get_variance(ar, images, control_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
